{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-10T13:18:23.555757Z",
     "iopub.status.idle": "2022-04-10T13:18:23.557902Z",
     "shell.execute_reply.started": "2022-04-10T13:18:23.557664Z",
     "shell.execute_reply": "2022-04-10T13:18:23.557692Z"
    },
    "trusted": true,
    "cell_id": "00002-167c1848-bcba-4836-8bd1-3ddc4ef082b4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ff12ffbb",
    "execution_start": 1650122930006,
    "execution_millis": 8976,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 791
   },
   "source": "# !pip3 install sklearn\n# !pip3 install tqdm\n# !pip3 install tensorboardX\n!pip3 install timm\n!pip3 install torchmetrics\n!pip3 install albumentations",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: timm in /root/venv/lib/python3.7/site-packages (0.5.4)\nRequirement already satisfied: torchvision in /shared-libs/python3.7/py/lib/python3.7/site-packages (from timm) (0.12.0)\nRequirement already satisfied: torch>=1.4 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from timm) (1.11.0)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from torchvision->timm) (4.1.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchvision->timm) (9.0.1)\nRequirement already satisfied: numpy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchvision->timm) (1.21.5)\nRequirement already satisfied: requests in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchvision->timm) (2.27.1)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->torchvision->timm) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests->torchvision->timm) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->torchvision->timm) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests->torchvision->timm) (3.3)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: torchmetrics in /root/venv/lib/python3.7/site-packages (0.8.0)\nRequirement already satisfied: pyDeprecate==0.3.* in /root/venv/lib/python3.7/site-packages (from torchmetrics) (0.3.2)\nRequirement already satisfied: packaging in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.3.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchmetrics) (1.11.0)\nRequirement already satisfied: numpy>=1.17.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchmetrics) (1.21.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.7)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: albumentations in /root/venv/lib/python3.7/site-packages (1.1.0)\nRequirement already satisfied: qudida>=0.0.4 in /root/venv/lib/python3.7/site-packages (from albumentations) (0.0.4)\nRequirement already satisfied: scikit-image>=0.16.1 in /root/venv/lib/python3.7/site-packages (from albumentations) (0.19.2)\nRequirement already satisfied: PyYAML in /shared-libs/python3.7/py/lib/python3.7/site-packages (from albumentations) (6.0)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /root/venv/lib/python3.7/site-packages (from albumentations) (4.5.5.64)\nRequirement already satisfied: numpy>=1.11.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from albumentations) (1.21.5)\nRequirement already satisfied: scipy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from albumentations) (1.7.3)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.1.1)\nRequirement already satisfied: scikit-learn>=0.19.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\nRequirement already satisfied: networkx>=2.2 in /root/venv/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\nRequirement already satisfied: PyWavelets>=1.1.1 in /root/venv/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\nRequirement already satisfied: imageio>=2.4.1 in /root/venv/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.16.2)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (9.0.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /root/venv/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\nRequirement already satisfied: joblib>=0.11 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5c6db781",
    "execution_start": 1650122938993,
    "execution_millis": 1420,
    "execution": {
     "iopub.status.busy": "2022-04-10T13:17:49.764587Z",
     "iopub.execute_input": "2022-04-10T13:17:49.764870Z",
     "iopub.status.idle": "2022-04-10T13:17:50.957957Z",
     "shell.execute_reply.started": "2022-04-10T13:17:49.764841Z",
     "shell.execute_reply": "2022-04-10T13:17:50.957030Z"
    },
    "trusted": true,
    "cell_id": "16027c27-e36b-43db-ae14-f77347475b56",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 334.375
   },
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch import optim\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorboardX import SummaryWriter\nfrom timm.utils import AverageMeter\nimport random\nimport torchmetrics",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "File paths to the data",
   "metadata": {
    "cell_id": "c0c469b487864c62bd7cac345ae51b33",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "305b546d5e1f47f7b631ae3c81ccfc69",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6d22b7dc",
    "execution_start": 1650122940417,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "# Kaggle\nannotation_file_path = \"../input/sorghum-id-fgvc-9/train_cultivar_mapping.csv\"\nimg_dir_path = \"../input/sorghum-id-fgvc-9/train_images\"\n\n# DeepNote\nannotation_file_path = \"data/train_cultivar_mapping.csv\"\nimg_dir_path = \"data/train_images\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "928ab6c23b4245df9a7b23c40a19483f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "77911824",
    "execution_start": 1650122940423,
    "execution_millis": 642,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1719
   },
   "source": "import pandas as pd\nimport albumentations as A\nfrom pathlib import Path\nfrom albumentations.core.composition import Compose, OneOf\nfrom albumentations.pytorch import ToTensorV2\n\nclass DatasetParams:\n    universe = pd.read_csv(Path(\"data\") / \"train_metadata_subset.csv\")\n    img_size = 512\n    test_size = 0.38 # change to 0.2 later\n    filepaths = {\n        'train': Path(\"data\") / \"train.csv\",\n        'test': Path(\"data\") / \"test.csv\"\n    }\n    imgdirpath = Path(\"data\") / \"train_images\"\n\n    # mean and std of each color channel\n    # below is that of imagenet \n    norm_mean = [0.485, 0.456, 0.406]\n    norm_std = [0.229, 0.224, 0.225]\n\nDP = DatasetParams\n\nDP.transforms = {\n    'train': Compose([\n                A.RandomResizedCrop(height=DP.img_size, width=DP.img_size),\n                A.Flip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.ShiftScaleRotate(p=0.5),\n                A.HueSaturationValue(p=0.5),\n                A.OneOf([\n                    A.RandomBrightnessContrast(p=0.5),\n                    A.RandomGamma(p=0.5),\n                ], p=0.5),\n                A.OneOf([\n                    A.Blur(p=0.1),\n                    A.GaussianBlur(p=0.1),\n                    A.MotionBlur(p=0.1),\n                ], p=0.1),\n                A.OneOf([\n                    A.GaussNoise(p=0.1),\n                    A.ISONoise(p=0.1),\n                    A.GridDropout(ratio=0.5, p=0.2),\n                    A.CoarseDropout(max_holes=16, min_holes=8, max_height=16, max_width=16, min_height=8, min_width=8, p=0.2)\n                ], p=0.2),\n                A.Normalize(\n                    mean=DP.norm_mean,\n                    std=DP.norm_std,\n                ),\n                ToTensorV2(),\n            ]),\n    \n    'test': Compose([\n                A.Resize(height=DP.img_size, width=DP.img_size),\n                A.Normalize(\n                    mean=DP.norm_mean,\n                    std=DP.norm_std,\n                ),\n                ToTensorV2(),\n            ])\n}\n\nDP.transforms_vis = {\n    'train': Compose([\n                A.RandomResizedCrop(height=DP.img_size, width=DP.img_size),\n                A.Flip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.ShiftScaleRotate(p=0.5),\n                A.HueSaturationValue(p=0.5),\n                A.OneOf([\n                    A.RandomBrightnessContrast(p=0.5),\n                    A.RandomGamma(p=0.5),\n                ], p=0.5),\n                A.OneOf([\n                    A.Blur(p=0.1),\n                    A.GaussianBlur(p=0.1),\n                    A.MotionBlur(p=0.1),\n                ], p=0.1),\n                A.OneOf([\n                    A.GaussNoise(p=0.1),\n                    A.ISONoise(p=0.1),\n                    A.GridDropout(ratio=0.5, p=0.2),\n                    A.CoarseDropout(max_holes=16, min_holes=8, max_height=16, max_width=16, min_height=8, min_width=8, p=0.2)\n                ], p=0.2),\n                ToTensorV2(),\n            ]),\n    \n    'test': Compose([\n                A.Resize(height=DP.img_size, width=DP.img_size),\n                ToTensorV2(),\n            ])\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Split data into train and test ",
   "metadata": {
    "cell_id": "f96a6b7ec7524ea0979f744987698cba",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a701fbb729144a1890211d39d9fd537b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a8bdaf3e",
    "execution_start": 1650122941083,
    "execution_millis": 755,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1493
   },
   "source": "from pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport random\n\nimg_labels = DP.universe\nprint(len(img_labels))\n\n# quick macro for dividing a database into two disjunct ones based on a function that returns boolean\ndef filter(dataframe, key, lb, yes, no):\n    for i, k in enumerate(dataframe[key].tolist()):\n        if lb(k):\n            yes.loc[len(yes)] = dataframe.loc[i]\n        else:\n            if no is not None:\n                no.loc[len(no)] = dataframe.loc[i]\n\ntrain = pd.DataFrame(columns=['image', 'cultivar'])\ntest = pd.DataFrame(columns=['image', 'cultivar'])\nimg_to_split = pd.DataFrame(columns=['image', 'cultivar'])\n\n# isolate all class with only 1 training sample and put them in train\ncounts = img_labels['cultivar'].value_counts()\nprint(counts)\nfilter(img_labels, 'cultivar', lambda cultivar: counts[cultivar] == 1, train, img_to_split)\n\nimgs = img_to_split['image'].tolist()\nlabels = img_to_split['cultivar'].tolist()\n\nprint(len(imgs), len(labels))\n\n# use train_test_split with stratify to split class with multiple training samples\ntrain_split, test_split = train_test_split(imgs, test_size=DP.test_size, stratify=labels)\nprint('train', train_split)\nprint('test_split', test_split, len(test_split))\n\nfilter(img_to_split, 'image', lambda image: image in train_split, train, test)\n\nprint('train', train)\nprint('test', test)\n\nassert(len(img_labels) == len(train) + len(test))\n\ntrain.to_csv(DP.filepaths['train'], index=False)\ntest.to_csv(DP.filepaths['test'], index=False)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "165\nPI_52606     6\nPI_175919    5\nPI_196049    5\nPI_154987    4\nPI_329338    4\n            ..\nPI_229841    1\nPI_329310    1\nPI_157804    1\nPI_152967    1\nPI_329301    1\nName: cultivar, Length: 83, dtype: int64\n133 133\ntrain ['2017-06-15__14-21-45-891.png', '2017-06-28__11-30-01-893.png', '2017-06-02__18-04-25-964.png', '2017-06-28__11-50-22-395.png', '2017-06-04__14-14-17-738.png', '2017-06-01__11-22-16-985.png', '2017-06-20__12-22-46-212.png', '2017-06-13__11-39-32-692.png', '2017-06-22__13-18-06-841.png', '2017-06-22__13-21-19-163.png', '2017-06-26__11-57-27-185.png', '2017-06-15__13-11-54-968.png', '2017-06-29__11-58-37-039.png', '2017-06-15__14-21-07-961.png', '2017-06-05__12-55-58-623.png', '2017-06-23__12-44-08-451.png', '2017-06-02__18-15-54-053.png', '2017-06-09__13-36-33-498.png', '2017-06-02__18-05-23-434.png', '2017-06-20__12-32-37-803.png', '2017-06-19__17-58-41-017.png', '2017-06-18__13-33-39-435.png', '2017-06-14__14-44-23-387.png', '2017-06-27__10-42-43-014.png', '2017-06-18__14-41-59-638.png', '2017-06-19__18-06-32-112.png', '2017-06-16__11-29-50-833.png', '2017-06-03__11-47-12-396.png', '2017-06-01__12-15-24-632.png', '2017-06-05__12-30-08-470.png', '2017-06-02__17-16-09-034.png', '2017-06-04__13-12-51-552.png', '2017-06-28__10-26-33-038.png', '2017-06-15__13-37-03-707.png', '2017-06-13__11-24-42-895.png', '2017-06-21__13-05-37-604.png', '2017-06-26__11-01-16-308.png', '2017-06-04__14-42-15-380.png', '2017-06-26__10-34-30-574.png', '2017-06-26__10-15-52-883.png', '2017-06-26__18-29-35-752.png', '2017-06-15__11-49-51-986.png', '2017-06-03__12-47-23-312.png', '2017-06-03__12-29-22-146.png', '2017-06-16__11-56-36-473.png', '2017-06-04__13-41-25-911.png', '2017-06-27__13-09-16-459.png', '2017-06-01__12-42-56-491.png', '2017-06-21__11-43-02-157.png', '2017-06-03__12-04-30-505.png', '2017-06-02__16-48-57-866.png', '2017-06-12__13-58-02-359.png', '2017-06-28__11-38-42-790.png', '2017-06-09__11-45-35-359.png', '2017-06-13__11-18-16-069.png', '2017-06-05__13-04-56-203.png', '2017-06-12__13-43-46-842.png', '2017-06-13__10-47-59-932.png', '2017-06-29__11-50-40-556.png', '2017-06-28__10-55-20-476.png', '2017-06-01__11-37-11-574.png', '2017-06-21__12-01-37-893.png', '2017-06-01__12-03-11-225.png', '2017-06-27__10-33-52-663.png', '2017-06-23__11-19-40-270.png', '2017-06-12__13-20-04-699.png', '2017-06-01__11-17-06-111.png', '2017-06-13__12-34-28-998.png', '2017-06-28__13-04-07-145.png', '2017-06-23__13-02-21-309.png', '2017-06-29__12-22-55-083.png', '2017-06-01__10-41-04-558.png', '2017-06-04__13-53-49-547.png', '2017-06-17__13-18-06-932.png', '2017-06-23__13-11-29-409.png', '2017-06-21__14-14-01-012.png', '2017-06-01__12-05-25-739.png', '2017-06-19__15-48-01-483.png', '2017-06-17__14-04-10-033.png', '2017-06-26__12-39-21-357.png', '2017-06-28__10-20-19-407.png', '2017-06-27__11-58-30-439.png']\ntest_split ['2017-06-26__12-56-48-642.png', '2017-06-19__17-31-15-431.png', '2017-06-12__13-38-25-248.png', '2017-06-15__14-08-45-158.png', '2017-06-16__12-24-20-930.png', '2017-06-15__12-16-02-755.png', '2017-06-04__13-53-43-915.png', '2017-06-23__19-06-38-489.png', '2017-06-09__11-56-23-546.png', '2017-06-17__14-20-22-298.png', '2017-06-04__12-47-42-879.png', '2017-06-20__11-56-31-082.png', '2017-06-18__14-51-24-016.png', '2017-06-27__10-34-57-557.png', '2017-06-17__13-19-07-185.png', '2017-06-16__10-55-21-396.png', '2017-06-20__13-17-16-857.png', '2017-06-16__10-52-35-266.png', '2017-06-22__12-09-18-775.png', '2017-06-13__11-28-55-689.png', '2017-06-26__11-11-17-559.png', '2017-06-02__17-58-56-793.png', '2017-06-30__11-52-46-639.png', '2017-06-20__12-22-35-820.png', '2017-06-19__15-57-59-480.png', '2017-06-28__11-29-25-953.png', '2017-06-01__11-53-29-066.png', '2017-06-16__11-51-23-009.png', '2017-06-26__11-56-32-325.png', '2017-06-09__11-39-51-960.png', '2017-06-29__11-23-48-156.png', '2017-06-26__12-53-06-632.png', '2017-06-29__10-47-20-695.png', '2017-06-03__12-46-54-011.png', '2017-06-20__12-05-04-039.png', '2017-06-03__12-59-57-532.png', '2017-06-29__11-50-31-536.png', '2017-06-12__13-16-47-112.png', '2017-06-21__13-02-59-731.png', '2017-06-09__13-06-18-145.png', '2017-06-13__11-18-54-600.png', '2017-06-22__12-33-07-526.png', '2017-06-17__12-58-37-901.png', '2017-06-15__13-29-23-723.png', '2017-06-13__11-55-55-043.png', '2017-06-15__12-04-20-182.png', '2017-06-28__12-40-56-393.png', '2017-06-21__11-34-51-977.png', '2017-06-12__13-18-07-707.png', '2017-06-15__13-40-45-092.png', '2017-06-09__13-36-14-892.png'] 51\ntrain                             image   cultivar\n0    2017-06-18__13-23-50-617.png  PI_329299\n1    2017-06-21__11-25-59-146.png  PI_329310\n2    2017-06-29__12-16-56-358.png  PI_154750\n3    2017-06-29__10-26-32-577.png  PI_213900\n4    2017-06-17__12-40-12-937.png  PI_152816\n..                            ...        ...\n109  2017-06-28__11-30-01-893.png  PI_167093\n110  2017-06-03__12-04-30-505.png  PI_257599\n111  2017-06-01__10-41-04-558.png  PI_152751\n112  2017-06-02__18-15-54-053.png   PI_52606\n113  2017-06-18__14-41-59-638.png  PI_152771\n\n[114 rows x 2 columns]\ntest                            image   cultivar\n0   2017-06-16__12-24-20-930.png  PI_257599\n1   2017-06-12__13-18-07-707.png   PI_92270\n2   2017-06-26__12-56-48-642.png  PI_176766\n3   2017-06-15__14-08-45-158.png   PI_52606\n4   2017-06-20__12-22-35-820.png  PI_273969\n5   2017-06-17__12-58-37-901.png   PI_22913\n6   2017-06-17__14-20-22-298.png  PI_156330\n7   2017-06-12__13-38-25-248.png  PI_170787\n8   2017-06-16__10-52-35-266.png  PI_152961\n9   2017-06-28__11-29-25-953.png  PI_266927\n10  2017-06-20__13-17-16-857.png  PI_221548\n11  2017-06-09__13-06-18-145.png  PI_152651\n12  2017-06-04__12-47-42-879.png  PI_273465\n13  2017-06-22__12-33-07-526.png  PI_146890\n14  2017-06-12__13-16-47-112.png  PI_152965\n15  2017-06-15__13-29-23-723.png  PI_156393\n16  2017-06-28__12-40-56-393.png  PI_329338\n17  2017-06-09__13-36-14-892.png  PI_197542\n18  2017-06-26__11-11-17-559.png  PI_156326\n19  2017-06-26__11-56-32-325.png   PI_52606\n20  2017-06-13__11-18-54-600.png  PI_196049\n21  2017-06-19__17-31-15-431.png  PI_154988\n22  2017-06-15__13-40-45-092.png  PI_152828\n23  2017-06-30__11-52-46-639.png  PI_329286\n24  2017-06-20__12-05-04-039.png  PI_329351\n25  2017-06-01__11-53-29-066.png  PI_156463\n26  2017-06-04__13-53-43-915.png  PI_152860\n27  2017-06-21__13-02-59-731.png  PI_196049\n28  2017-06-26__12-53-06-632.png   PI_35038\n29  2017-06-29__11-23-48-156.png  PI_145619\n30  2017-06-23__19-06-38-489.png  PI_195754\n31  2017-06-09__11-39-51-960.png   PI_19770\n32  2017-06-15__12-04-20-182.png  PI_175919\n33  2017-06-15__12-16-02-755.png  PI_155760\n34  2017-06-09__11-56-23-546.png  PI_329300\n35  2017-06-16__10-55-21-396.png  PI_152728\n36  2017-06-03__12-46-54-011.png  PI_196586\n37  2017-06-18__14-51-24-016.png  PI_154987\n38  2017-06-22__12-09-18-775.png  PI_329333\n39  2017-06-17__13-19-07-185.png  PI_152730\n40  2017-06-29__10-47-20-695.png  PI_175919\n41  2017-06-16__11-51-23-009.png  PI_152862\n42  2017-06-27__10-34-57-557.png  PI_145633\n43  2017-06-29__11-50-31-536.png  PI_152694\n44  2017-06-02__17-58-56-793.png  PI_156178\n45  2017-06-20__11-56-31-082.png  PI_297155\n46  2017-06-03__12-59-57-532.png  PI_217691\n47  2017-06-13__11-55-55-043.png  PI_253986\n48  2017-06-13__11-28-55-689.png  PI_167093\n49  2017-06-21__11-34-51-977.png  PI_251672\n50  2017-06-19__15-57-59-480.png  PI_152771\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Define the dataset",
   "metadata": {
    "cell_id": "a56d80d788d546529c79b736719b2427",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "be3ef2d68d8949c39ea4c364b88f9bde",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cf9d8389",
    "execution_start": 1650122941844,
    "execution_millis": 40,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1601
   },
   "source": "import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import convert_image_dtype\nimport cv2\nfrom sklearn import preprocessing\n\nclass CultivarDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n        self.labelenc = preprocessing.LabelEncoder()\n        self.labelenc.fit(self.img_labels['cultivar'].tolist())      \n\n    def to_label(self, string: str):\n        return self.labelenc.transform([string])[0]  \n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n\n        # NOTE: when pytorch reads an image, it is immediately transformed into a uint8 Tensor with each channel ranging in [0, 255]\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        label = self.to_label(self.img_labels.iloc[idx, 1])\n\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.target_transform:\n            label = self.target_transform(label)\n        \n        return image, label\n\n# small test to make sure it's working as intended\nc = CultivarDataset(\n    annotations_file=DP.filepaths['train'],\n    img_dir=DP.imgdirpath\n)\n\nprint(c.to_label('PI_257599'))\nprint(c[0])\nprint(f'Make sure there is no garbage data: {c.img_labels[c.img_labels[\"image\"].str.contains(\"2017\")==False]}')\n    ",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "59\n(array([[[112, 104,  58],\n        [103,  99,  59],\n        [104,  96,  59],\n        ...,\n        [ 72,  68,  47],\n        [ 69,  67,  44],\n        [ 52,  72,  63]],\n\n       [[102, 107,  72],\n        [102, 104,  72],\n        [101, 109,  76],\n        ...,\n        [ 61,  67,  54],\n        [ 62,  65,  54],\n        [ 47,  66,  63]],\n\n       [[110, 118,  82],\n        [107, 117,  88],\n        [102, 113,  89],\n        ...,\n        [ 61,  65,  53],\n        [ 61,  61,  56],\n        [ 46,  67,  70]],\n\n       ...,\n\n       [[119, 135, 112],\n        [117, 134, 111],\n        [123, 136, 114],\n        ...,\n        [ 96, 102,  81],\n        [ 94, 107,  82],\n        [ 71, 108, 109]],\n\n       [[121, 138, 114],\n        [120, 136, 113],\n        [120, 137, 111],\n        ...,\n        [ 94, 103,  81],\n        [ 96, 107,  84],\n        [ 74, 113, 108]],\n\n       [[103, 135, 133],\n        [102, 136, 132],\n        [101, 136, 126],\n        ...,\n        [ 79, 101,  94],\n        [ 82, 107,  99],\n        [ 62, 108, 127]]], dtype=uint8), 71)\nMake sure there is no garbage data: Empty DataFrame\nColumns: [image, cultivar]\nIndex: []\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4a79c158aee34ee482231e98a93d1fda",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a03c797a",
    "execution_start": 1650122941931,
    "execution_millis": 45,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 953
   },
   "source": "datasets = {\n    'train': CultivarDataset(\n                annotations_file=DP.filepaths['train'],\n                img_dir=DP.imgdirpath,\n                transform=DP.transforms['train']\n            ),\n    'test': CultivarDataset(\n                annotations_file=DP.filepaths['test'],\n                img_dir=DP.imgdirpath,\n                transform=DP.transforms['test']\n            )\n}\n\nprint(datasets['train'][0])\nprint(datasets['test'][0])",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "(tensor([[[-0.2513, -0.3541, -0.3198,  ..., -1.2274, -1.2617, -1.2788],\n         [-0.6109, -0.5424, -0.4054,  ..., -1.2617, -1.2617, -1.2445],\n         [-0.7993, -0.6623, -0.5253,  ..., -1.2274, -1.2274, -1.2445],\n         ...,\n         [ 0.0741,  0.0741,  0.0741,  ...,  0.2111,  0.1254,  0.0741],\n         [ 0.0912,  0.0912,  0.0912,  ..., -0.0287, -0.0801, -0.1143],\n         [ 0.0912,  0.0912,  0.0912,  ...,  0.0056, -0.0629, -0.0972]],\n\n        [[-0.3200, -0.3550, -0.3550,  ..., -1.0378, -1.0903, -1.0903],\n         [-0.5126, -0.4776, -0.4251,  ..., -1.1253, -1.1253, -1.1078],\n         [-0.6877, -0.6001, -0.4951,  ..., -1.1078, -1.1078, -1.1078],\n         ...,\n         [ 0.4678,  0.4678,  0.4853,  ...,  0.4853,  0.4153,  0.3803],\n         [ 0.4853,  0.4853,  0.4853,  ...,  0.2752,  0.2402,  0.2227],\n         [ 0.4853,  0.4853,  0.4853,  ...,  0.3102,  0.2577,  0.2402]],\n\n        [[-0.5495, -0.5670, -0.5495,  ..., -1.0027, -1.0376, -1.0376],\n         [-0.6367, -0.6367, -0.6193,  ..., -1.0550, -1.0724, -1.0724],\n         [-0.7936, -0.7238, -0.6541,  ..., -1.0724, -1.0724, -1.0724],\n         ...,\n         [ 0.1825,  0.1825,  0.1825,  ...,  0.0605, -0.0092, -0.0441],\n         [ 0.1825,  0.1825,  0.1825,  ..., -0.1138, -0.1312, -0.1487],\n         [ 0.1825,  0.1825,  0.1651,  ..., -0.0964, -0.1312, -0.1487]]]), 71)\n(tensor([[[-0.5767, -0.6965, -0.7137,  ..., -1.0048, -1.0048, -0.9363],\n         [-0.5596, -0.7993, -0.7822,  ..., -0.9877, -1.0390, -0.8335],\n         [-0.5767, -0.7308, -0.7308,  ..., -1.0048, -0.8335, -0.4911],\n         ...,\n         [-0.7993, -0.9705, -1.0048,  ..., -0.7308, -0.7479, -0.7308],\n         [-0.8507, -1.0048, -1.0048,  ..., -0.6623, -0.6281, -0.5767],\n         [-0.9705, -1.0733, -1.0733,  ..., -0.8678, -0.8849, -0.9877]],\n\n        [[-0.7927, -0.4951, -0.5126,  ..., -0.8803, -0.8627, -0.7927],\n         [-0.4776, -0.4951, -0.4951,  ..., -0.8627, -0.8452, -0.6527],\n         [-0.4601, -0.4601, -0.4426,  ..., -0.7927, -0.6877, -0.2150],\n         ...,\n         [-0.7577, -0.8102, -0.7752,  ..., -0.3901, -0.4076, -0.4251],\n         [-0.7752, -0.7927, -0.7927,  ..., -0.3025, -0.3025, -0.2850],\n         [-0.7927, -0.7927, -0.7927,  ..., -0.5476, -0.5826, -0.6176]],\n\n        [[-0.9678, -0.7064, -0.7064,  ..., -0.9156, -0.8981, -0.8284],\n         [-0.6715, -0.6367, -0.6367,  ..., -0.7936, -0.7936, -0.6367],\n         [-0.6890, -0.5495, -0.5495,  ..., -0.7587, -0.6367, -0.2532],\n         ...,\n         [-1.0027, -0.8633, -0.9156,  ..., -0.6890, -0.7064, -0.6541],\n         [-1.0201, -0.8807, -0.8807,  ..., -0.6541, -0.6541, -0.6193],\n         [-0.9504, -0.8458, -0.7936,  ..., -0.7238, -0.7238, -0.7936]]]), 35)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e26c063b",
    "execution_start": 1650122941976,
    "execution_millis": 1478,
    "execution": {
     "iopub.status.busy": "2022-04-10T13:17:58.549192Z",
     "iopub.execute_input": "2022-04-10T13:17:58.549858Z",
     "iopub.status.idle": "2022-04-10T13:17:58.827275Z",
     "shell.execute_reply.started": "2022-04-10T13:17:58.549819Z",
     "shell.execute_reply": "2022-04-10T13:17:58.826612Z"
    },
    "trusted": true,
    "cell_id": "00003-39870fad-6656-4e5f-90be-24b56652293e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 417.1875,
    "deepnote_output_heights": [
     21.1875,
     250
    ]
   },
   "source": "data = pd.read_csv(annotation_file_path)\ndata.cultivar.value_counts().hist()",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "metadata": {}
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 368,
       "height": 248
      }
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Notes on Hyperparameter:\n- https://arxiv.org/pdf/1803.09820.pdf\n- Momentum is usually always 0.9",
   "metadata": {
    "tags": [],
    "cell_id": "00006-9f1b17d8-aca6-496a-a653-0728b41aae5f",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 128.171875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "09ff89f1afac470a950fa6b89ac400cc",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "45a2a773",
    "execution_start": 1650122943169,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 351
   },
   "source": "class CustomEffNet(nn.Module):\n    def __init__(self, model_name='tf_efficientnet_b0_ns', pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.model.get_classifier().in_features\n#         self.model.fc = nn.Linear(in_features, CFG.num_classes)\n        self.model.classifier = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(in_features, 100)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fba592ed",
    "execution_start": 1650122943170,
    "execution_millis": 0,
    "execution": {
     "iopub.status.busy": "2022-04-10T13:17:58.926419Z",
     "iopub.execute_input": "2022-04-10T13:17:58.928301Z",
     "iopub.status.idle": "2022-04-10T13:17:58.933793Z",
     "shell.execute_reply.started": "2022-04-10T13:17:58.928269Z",
     "shell.execute_reply": "2022-04-10T13:17:58.931502Z"
    },
    "trusted": true,
    "cell_id": "00007-44ddf17f-5f66-44d2-9265-6dc819110937",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "# Hyperparameters\nlr = 1e-4\nmax_lr = 1e-3\nmomentum = 0.9\nweight_decay = 1e-5\nepoches = 40",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "58385167",
    "execution_start": 1650122943170,
    "execution_millis": 0,
    "execution": {
     "iopub.status.busy": "2022-04-10T13:17:58.934868Z",
     "iopub.execute_input": "2022-04-10T13:17:58.935114Z",
     "iopub.status.idle": "2022-04-10T13:17:58.942549Z",
     "shell.execute_reply.started": "2022-04-10T13:17:58.935079Z",
     "shell.execute_reply": "2022-04-10T13:17:58.941214Z"
    },
    "trusted": true,
    "cell_id": "00008-1d2fa34e-5324-483c-9cf1-b3ce2e15c4d4",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 207
   },
   "source": "def set_random_seed(seed=0, deterministic=False):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    if deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2721b579",
    "execution_start": 1650122943171,
    "execution_millis": 1,
    "execution": {
     "iopub.status.busy": "2022-04-10T13:17:58.944252Z",
     "iopub.execute_input": "2022-04-10T13:17:58.944567Z",
     "iopub.status.idle": "2022-04-10T13:17:58.952672Z",
     "shell.execute_reply.started": "2022-04-10T13:17:58.944532Z",
     "shell.execute_reply": "2022-04-10T13:17:58.951826Z"
    },
    "trusted": true,
    "cell_id": "00009-b6274abe-8654-4055-b1a3-59c7ccf05eb4",
    "owner_user_id": "9d557d55-fc23-4c8b-b1ac-104ba3539c98",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 585
   },
   "source": "def cultivar_train(model, data_loader_train, optimizer, scheduler, criterion, metric, epoch, summary_writer):\n    model.train()\n    optimizer.zero_grad()\n    loss_meter = AverageMeter()\n    acc_meter = AverageMeter()\n    # total_samples, correct = 0, 0\n    with tqdm(enumerate(data_loader_train), total=len(data_loader_train)) as pbar:\n        for idx, (samples, targets) in pbar:\n            samples = samples.cuda()\n            targets = targets.cuda()\n            out = model(samples)\n            pred = F.softmax(out)\n            loss = criterion(pred, targets)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            loss_meter.update(loss.item(), targets.size(0))\n\n            # Calculate accuracy\n            acc = metric(pred.argmax(1).cpu(), targets.cpu())\n            acc_meter.update(acc, 1)\n            # _, predicted = torch.max(pred.data, 1)\n            # total_samples += targets.size(0)\n            # correct += (predicted == targets).sum().item()\n            if idx%10==0:\n                summary_writer.add_scalar(f'lr', scheduler.get_last_lr(), epoch*len(data_loader_train)+id)\n            pbar.set_description(f\"Train epoch {epoch}, loss: {loss: .4f}, accuracy: {acc: .4f}\")\n\n    return loss_meter.avg, acc_meter.avg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3e95c8d",
    "execution_start": 1650122943174,
    "execution_millis": 2,
    "execution": {
     "iopub.status.busy": "2022-04-10T13:17:58.955090Z",
     "iopub.execute_input": "2022-04-10T13:17:58.955441Z",
     "iopub.status.idle": "2022-04-10T13:17:58.963978Z",
     "shell.execute_reply.started": "2022-04-10T13:17:58.955405Z",
     "shell.execute_reply": "2022-04-10T13:17:58.963103Z"
    },
    "trusted": true,
    "cell_id": "00010-538d1048-c03b-48e4-aedb-f228dbb9a9b5",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 477
   },
   "source": "def cultivar_val(model, data_loader_val, criterion, metric, epoch):\n    model.eval()\n    loss_meter = AverageMeter()\n    acc_meter = AverageMeter()\n    # total_samples, correct = 0, 0\n    with tqdm(enumerate(data_loader_val), total=len(data_loader_val)) as pbar:\n        for idx, (samples, targets) in pbar:\n            samples = samples.cuda()\n            targets = targets.cuda()\n            out = model(samples)\n            pred = F.softmax(out)\n            loss = criterion(pred, targets)\n            loss_meter.update(loss.item(), targets.size(0))\n            \n            # Calculate accuracy\n            acc = metric(pred.argmax(1).cpu(), targets.cpu())\n            acc_meter.update(acc, 1)\n            # _, predicted = torch.max(pred.data, 1)\n            # total_samples += targets.size(0)\n            # correct += (predicted == targets).sum().item()\n            pbar.set_description(f\"Validation epoch {epoch}, loss: {loss: .4f}, accuracy: {acc: .4f}\")\n    \n    return loss_meter.avg, acc_meter.avg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "You need to run the `test.csv` and `train.csv` generation in `cultivardataset.ipynb` before running this!",
   "metadata": {
    "tags": [],
    "cell_id": "00011-1a3bd369-87e5-4f08-9715-de6f6c27fdb5",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.78125
   }
  },
  {
   "cell_type": "markdown",
   "source": "\nThe general procedure of k-fold is as follows:\n\n- Shuffle the dataset randomly.\n- Split the dataset into k groups\n- For each unique group:\n    - Take the group as a hold out or test data set\n    - Take the remaining groups as a training data set\n    - Fit a model on the training set and evaluate it on the test set\n    - Retain the evaluation score and discard the model\n- Summarize the skill of the model using the sample of model evaluation scores",
   "metadata": {
    "tags": [],
    "cell_id": "00012-8237d3ec-d9d8-49c9-a981-924713b2f8bd",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 291.515625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4c858027",
    "execution_start": 1650122943226,
    "execution_millis": 2118,
    "execution": {
     "iopub.status.busy": "2022-04-10T13:17:58.965384Z",
     "iopub.execute_input": "2022-04-10T13:17:58.965880Z",
     "iopub.status.idle": "2022-04-10T13:18:23.541213Z",
     "shell.execute_reply.started": "2022-04-10T13:17:58.965845Z",
     "shell.execute_reply": "2022-04-10T13:18:23.537890Z"
    },
    "trusted": true,
    "cell_id": "00013-ebdf3505-2169-4b1c-b983-745b237c2827",
    "owner_user_id": "936eb0aa-03ca-4152-ac6e-ea9f3d0f9c5e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1507.6875,
    "deepnote_output_heights": [
     null,
     40.375
    ]
   },
   "source": "import timm\n\nset_random_seed(seed=42)\nprint(\"Creating datasets...\")\n\nsummary_writer = SummaryWriter()\n\nprint(\"Validation dataset created\")\ndata_loader_train = torch.utils.data.DataLoader(\n    datasets['train'],\n    batch_size = 16,\n    num_workers = 2,\n    shuffle=True,\n    pin_memory = True,\n    drop_last = True\n)\n\ndata_loader_val = torch.utils.data.DataLoader(\n    datasets['test'],\n    batch_size = 16,\n    num_workers = 2,\n    shuffle=False,\n    pin_memory = True,\n    drop_last = False\n)\n\nprint(\"Dataloader created\")\nprint(\"Creating model...\")\nmodel = CustomEffNet(model_name='tf_efficientnet_b3_ns', pretrained=True)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n                                                        epochs=epoches, steps_per_epoch=len(data_loader_train),\n                                                        max_lr=max_lr, pct_start=0.2, \n                                                        div_factor=1e3, final_div_factor=1e3)\ncriterion = nn.CrossEntropyLoss()\nmetric = torchmetrics.Accuracy(threshold=0.5, num_classes=100)\n\nmin_loss = float('inf')\n\nprint(\"Start Training\")\nfor epoch in range(epoches):\n    loss_train, acc_train = cultivar_train(model, data_loader_train, optimizer, scheduler, criterion, metric, epoch, summary_writer)\n    loss_val, acc_val = cultivar_val(model, data_loader_val, criterion, metric, epoch)\n    min_loss = min(min_loss, loss_val)\n    if min_loss == loss_val:\n        save_state = {'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'epoch': epoch\n                }\n\n        save_path = f'ckpt_epoch_{epoch}.pth'\n        print(f\"{save_path} saving...\")\n        torch.save(save_state, save_path)\n        print(f\"{save_path} saved\")\n\n    print(\"Writing to summarywriter...\")\n    summary_writer.add_scalar(f'Loss/train', loss_train, epoch)\n    summary_writer.add_scalar(f'Loss/val', loss_val, epoch)\n    summary_writer.add_scalar(f'Acc/train', acc_train, epoch)\n    summary_writer.add_scalar(f'Acc/val', acc_val, epoch)\n    summary_writer.add_scalar(f'Min_loss', min_loss, epoch)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Creating datasets...\nValidation dataset created\nDataloader created\nCreating model...\nStart Training\n  0%|          | 0/7 [00:00<?, ?it/s]",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d841b2e3-7f2f-42e6-ae8e-6cea1c0a3631' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "deepnote_notebook_id": "02263764-213b-44c9-a49d-dd6ecb481d8f",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}